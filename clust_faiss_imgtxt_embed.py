# generate_openclip_faiss_index_with_kmeans.py

import os
import torch
import faiss
import open_clip
from PIL import Image
from tqdm import tqdm
from sklearn.cluster import KMeans
import numpy as np
import json

# âœ… í™˜ê²½ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… OpenCLIP ëª¨ë¸ ë¡œë“œ
model, preprocess, _ = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')
model = model.to(device)
tokenizer = open_clip.get_tokenizer('ViT-H-14')

# âœ… ê²½ë¡œ ì„¤ì •
image_root_dir = r"C:/í¬ë¡¤ë§ ìµœì¢…/all data"  # ğŸ”¥ ì´ë¯¸ì§€ í´ë”
embedding_save_path = r"C:/Users/user/Desktop/clust_faiss_imgtxt.pth"
faiss_index_save_path = r"C:/Users/user/Desktop/clust_faiss_imgtxt.index"
cluster_save_path = r"C:/Users/user/Desktop/clust_kmeans_clusters.json"

# âœ… ì„ë² ë”© ìƒì„±
features = []
image_paths = []

image_files = [f for f in os.listdir(image_root_dir) if f.lower().endswith((".jpg", ".jpeg", ".png"))]

for filename in tqdm(image_files, desc="OpenCLIP ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘", ncols=80, dynamic_ncols=True):
    img_path = os.path.join(image_root_dir, filename)
    txt_path = os.path.splitext(img_path)[0] + ".txt"  # ê°™ì€ ì´ë¦„ì˜ .txt íŒŒì¼ ì°¾ê¸°

    try:
        # âœ… ì´ë¯¸ì§€ ì„ë² ë”©
        image = Image.open(img_path).convert("RGB")
        image_tensor = preprocess(image).unsqueeze(0).to(device)
        with torch.no_grad():
            image_feat = model.encode_image(image_tensor)
        image_feat = image_feat / image_feat.norm(dim=-1, keepdim=True)

        # âœ… í…ìŠ¤íŠ¸ ì„ë² ë”© (ì—†ìœ¼ë©´ 0ë²¡í„° ëŒ€ì²´)
        if os.path.exists(txt_path):
            with open(txt_path, "r", encoding="utf-8") as f:
                text = f.read().strip()
            text_tokens = tokenizer([text]).to(device)
            with torch.no_grad():
                text_feat = model.encode_text(text_tokens)
            text_feat = text_feat / text_feat.norm(dim=-1, keepdim=True)
        else:
            text_feat = torch.zeros((1, model.visual.output_dim)).to(device)

        # âœ… ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ê²°í•© (í‰ê· )
        combined_feat = (image_feat + text_feat) / 2
        features.append(combined_feat.squeeze(0).cpu())
        image_paths.append(img_path)

    except Exception as e:
        tqdm.write(f"âŒ ì‹¤íŒ¨: {img_path} ({str(e)})")
        continue

# âœ… ì €ì¥
if features:
    final_tensor = torch.stack(features)

    # 1ï¸âƒ£ ì„ë² ë”©ê³¼ ì´ë¯¸ì§€ ê²½ë¡œ ì €ì¥
    torch.save({"features": final_tensor, "paths": image_paths}, embedding_save_path)
    print(f"\nğŸ‰ ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì„ë² ë”© ì €ì¥ ì™„ë£Œ â†’ {embedding_save_path}")

    # 2ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
    dim = final_tensor.shape[1]
    index = faiss.IndexFlatIP(dim)  # ë‚´ì  ê¸°ë°˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    normalized_features = final_tensor / final_tensor.norm(dim=-1, keepdim=True)
    index.add(normalized_features.numpy())
    faiss.write_index(index, faiss_index_save_path)
    print(f"ğŸ‰ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ â†’ {faiss_index_save_path}")

    # 3ï¸âƒ£ KMeans í´ëŸ¬ìŠ¤í„°ë§
    n_clusters = 13  # ğŸ”§ í•„ìš”ì— ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(final_tensor.numpy())

    # âœ… í´ëŸ¬ìŠ¤í„°ë³„ ê²°ê³¼ ì €ì¥
    cluster_result = {}
    for label, path in zip(cluster_labels, image_paths):
        cluster_result.setdefault(int(label), []).append(path)

    with open(cluster_save_path, "w", encoding="utf-8") as f:
        json.dump(cluster_result, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“Š KMeans í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {cluster_save_path}")
else:
    print("âŒ ìƒì„±ëœ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
